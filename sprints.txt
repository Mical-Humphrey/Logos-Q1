Phase 2
========

Implementation Plan
- Order generation: build the strategy-to-order translator that respects symbol metadata, precision, notional caps, and sizing rules.
- Paper broker completion: implement deterministic fills, FIFO inventory, realized/unrealized PnL, order-state lifecycle, and event logging.
- Data feeds: deliver minute-bar feeds (cache replay for tests, yfinance polling for equities, CCXT public polling for crypto) with freshness checks and retry/backoff.
- Risk and circuit breakers: wire max-notional, exposure, drawdown, reject, stale-data, and kill-switch guards into the runner with focused tests.
- State and reporting: persist state.json/state.jsonl for restart support, reconcile open orders, and emit full session artifacts (orders.csv, trades.csv, positions.csv, account.csv, session.md).
- Runner integration: connect the completed components and prove an end-to-end paper session using deterministic fixtures.
- Broker adapters: finalize CCXT and Alpaca connectors for dry runs and credential validation, leaving IB as a documented stub.
- Docs and runbook: refresh README, manual, and live runbook to reflect the finished live workflow.
- Test suite: keep `pytest -q` passing with the new live-mode coverage and without network credentials.

Definition of Done
- `python -m logos.live trade ...` runs end-to-end in paper mode using deterministic feeds, produces the full session artifact set, and exercises all risk guards (kill switch, breaker thresholds, restart flow) with test coverage.
- Strategy order generation reuses backtest signals, emitting quantized `OrderIntent`s that respect market metadata and sizing caps.
- Paper broker, risk module, and data feeds are covered by unit tests; CCXT and Alpaca adapters validate configuration and support dry-run execution paths.
- The config validator blocks unsafe live runs and verifies credential readiness for live adapters.
- Documentation and runbook describe the live workflow, safety checklist, and recovery steps.
- Full test suite passes locally without relying on external networks or real credentials.

Sprint Plan

Sprint A (Core Loop Completion)
- Implement and test strategy-to-order translation.
- Finish paper broker fill logic, inventory tracking, and PnL accounting.
- Build deterministic minute-bar feeds (cache replay and mockable providers), including freshness checks and retries.
- Integrate risk/circuit breakers with targeted tests.
- Finalize state persistence, restart reconciliation, and artifact writers.
- Demonstrate an end-to-end paper session using deterministic inputs, producing the expected artifacts.

Sprint B (Adapters & Polish)
- Complete CCXT and Alpaca dry-run adapters with credential validation and error handling; document IB as a future task.
- Expand reporting (session markdown, summaries) and ensure config validator enforces safety toggles.
- Refresh documentation/runbook to match the finished live pipeline.
- Harden tests and CI workflows around live-mode features, ensuring `pytest` stays green in isolation.



Phase 3 — Visual Dashboard (Streamlit + Plotly)
===============================================

Goal
- Build a read-only Streamlit dashboard that surfaces backtests, live sessions, metrics, and logs from the filesystem. No trading actions in Phase 3 (view-only). The UI must be modular, fast, and safe to run locally.

Scope
- Use Streamlit for the GUI, Plotly for charts, and import canonical paths from logos.paths (add any missing helpers).
- Read artifacts from runs/, data/cache/, and logos/logs/ only; never mutate artifacts from the UI.

Implementation Plan (Read-only UI)
- Package: logos/ui/streamlit/
  - __init__.py
  - app.py (main entry; streamlit run logos/ui/streamlit/app.py)
  - state.py (session_state: selected symbol/run, refresh interval, theme)
  - data_access.py (pure readers using logos.paths; legacy-aware)
  - components/
    - metrics_card.py (CAGR, Sharpe, MaxDD, WinRate, Exposure)
    - equity_chart.py (Plotly equity + drawdown overlay)
    - trades_table.py (paginated trades view with filters/download)
    - log_viewer.py (tail file with autorefresh + regex filter)
  - pages/
    - 00_Overview.py (KPI tiles, recent runs, latest live session)
    - 10_Backtests.py (run explorer; compare runs)
    - 20_Live_Monitor.py (session status, PnL, positions, fills, log tail)
    - 30_Strategy_Lab.py (strategy docs/param schema; no execute)
    - 40_Settings.py (safe .env/config view; theme + refresh)
    - (Optional) 50_Tutor_Viewer.py (render lessons transcript/plots)

Paths & Files (read-only; align with Phase 2)
- Backtests: runs/<backtest_id>/{metrics.json, trades.csv, equity.png, config.yaml, logs/run.log}
  - Legacy equity images: runs/equity_{SYMBOL}_{STRAT}.png (display if present)
- Live: runs/live/sessions/<id>/{orders.csv, trades.csv, positions.csv, account.csv, session.md, plots/*.png, logs/run.log, state.json}
  - runs/live/latest_session (symlink or pointer file)
- Logs: logos/logs/live.log and logos/logs/app.log
- Tutor: runs/lessons/<lesson>/<timestamp>/{transcript.md|.txt, glossary.json, plots/*.png}

Performance & Quality
- Backtests page loads under 500ms for typical runs (<50k rows).
- Live Monitor avoids full rereads; cache by mtime and use small deltas.
- Handle partial file writes without crashing; show retry/empty-state.
- Accessibility: responsive layout, keyboard navigation, alt text, consistent colors.
- Security: never display secrets; redact any values with KEY/SECRET/TOKEN.

Testing
- tests/ui/test_data_access.py: validate loaders on fixtures in tests/fixtures/runs/…
- tests/ui/test_pages_import.py: import all pages and assert layout functions render without raising using fixtures/mocks.

Documentation
- docs/DASHBOARD.md: how to run, screenshots per page, file/metrics expectations and empty-state behavior.
- Update README.md and docs/MANUAL.html with “Phase 3 Dashboard” section and a short gif/screenshot.

Definition of Done (Phase 3)
- streamlit run logos/ui/streamlit/app.py launches without errors.
- Overview shows KPIs, recent backtests, and (if present) latest live session with PnL and log tail.
- Backtests page: run picker, metrics cards, equity with drawdown, trades table with filters/download, compare mode with overlay + metric deltas.
- Live Monitor: with a dummy/recorded session fixture, displays account snapshot, positions, trades, and live.log tail; autorefresh updates PnL chart and log tail.
- Strategy Lab lists strategies from logos/strategies and renders docstrings/param schemas and example CLIs.
- Settings shows redacted env/config and allows changing UI refresh interval and theme.
- Missing paths or empty runs produce clear empty-state cards—no stack traces.
- Tests pass: pytest -q (UI tests + existing suite).
- Docs updated: docs/DASHBOARD.md plus README & MANUAL references.

Smaller, Focused Sprints (1–2 days each)

Sprint C0 — Prep & Skeleton
- Tasks:
  - Add Streamlit and Plotly to requirements if missing.
  - Scaffold logos/ui/streamlit/{__init__.py, app.py, state.py, data_access.py, components/, pages/}.
  - Ensure logos.paths exposes helpers for RUNS_DIR, LOGOS_DIR/logs, etc.
- Acceptance:
  - streamlit run logos/ui/streamlit/app.py shows a placeholder home page.
  - Imports from logos.paths succeed.

Sprint C1 — Data Access: Backtests (Base)
- Tasks:
  - Implement list_backtests(), load_backtest_metrics(), load_backtest_equity().
  - Handle standardized and legacy filenames; warn on legacy.
- Acceptance:
  - A simple script loads metrics/equity for a fixture run without errors.
  - Unit tests for these functions pass.

Sprint C2 — Data Access: Trades + Live Stubs
- Tasks:
  - Implement load_backtest_trades().
  - Stub list_live_sessions() and load_live_snapshot() returning empty frames when absent.
- Acceptance:
  - Trades load for fixture; live stubs return empty-state safely.
  - Tests updated and passing.

Sprint C3 — Components v1: Metrics + Equity
- Tasks:
  - Build components/metrics_card.py (CAGR, Sharpe, MaxDD, WinRate, Exposure).
  - Build components/equity_chart.py with drawdown overlay (Plotly).
- Acceptance:
  - Demo page renders metrics cards and equity chart from fixtures under 500ms.

Sprint C4 — Components v2: Trades Table + Log Viewer
- Tasks:
  - Build trades_table.py (pagination, side/date filters, CSV download).
  - Build log_viewer.py (tail last N lines with optional regex filter).
- Acceptance:
  - Demo page shows filtered trades; download button saves CSV; log tail works on a sample log.

Sprint C5 — Page: 00_Overview (MVP)
- Tasks:
  - KPI tiles: # backtests, # strategies (from filesystem), last run time.
  - Recent backtests table.
  - Latest live session panel (empty-state if none).
- Acceptance:
  - Overview renders with fixtures; empty-state shows friendly text.

Sprint C6 — Page: 10_Backtests (Single Run)
- Tasks:
  - Sidebar filters (symbol/strategy/date) and run picker.
  - Show metrics cards + equity chart for selected run.
- Acceptance:
  - Selecting a run updates cards and chart; no compare yet.

Sprint C7 — Page: 10_Backtests (Compare Mode)
- Tasks:
  - Add “Compare runs” toggle; select Run A and Run B.
  - Overlay equities; show side-by-side metric deltas.
- Acceptance:
  - Overlay works smoothly; metrics deltas are correct for fixtures.

Sprint C8 — Live Data Access (Real)
- Tasks:
  - Implement list_live_sessions() and load_live_snapshot() using Phase 2 paths (orders.csv, trades.csv, positions.csv, account.csv).
  - Implement tail_log() with safe partial-read handling; mtime-based caching.
- Acceptance:
  - With live fixtures, returns data frames and log lines quickly and safely.

Sprint C9 — Page: 20_Live_Monitor (MVP)
- Tasks:
  - Panels: account snapshot, open positions, recent fills, log tail.
  - Autorefresh interval in state (2/5/10s); show last refresh time.
- Acceptance:
  - Live page updates panels with fixtures; no PnL chart yet.

Sprint C10 — Live Monitor: PnL Timeline
- Tasks:
  - Compute PnL time series from trades/positions; Plotly line.
  - Cache with short TTL; avoid full rereads each tick.
- Acceptance:
  - PnL timeline updates on refresh; performance under 500ms with fixtures.

Sprint C11 — Page: 30_Strategy_Lab
- Tasks:
  - Enumerate strategies from logos/strategies; render docstrings and param schema; example CLIs.
- Acceptance:
  - Strategies list appears; params render; no execute button.

Sprint C12 — Page: 40_Settings
- Tasks:
  - Show safe subset of .env and logos.config (redact KEY/SECRET/TOKEN).
  - Theme toggle and refresh interval controls.
- Acceptance:
  - Redactions verified; state persists within session.

Sprint C13 — (Optional) 50_Tutor_Viewer
- Tasks:
  - Render transcript (.md or .txt) and plots from runs/lessons/<lesson>/<ts>/.
  - Graceful empty-state if lessons absent.
- Acceptance:
  - Viewer displays lesson artifacts for fixtures.

Sprint C14 — Caching & Partial-Write Hardening
- Tasks:
  - st.cache_data with TTLs (2–5s live); invalidate by file mtime.
  - Defensive reads (retry-on-partial, consistent parsing).
- Acceptance:
  - No crashes during simulated file writes; logs show retries.

Sprint C15 — Accessibility & Layout Polish
- Tasks:
  - Responsive layout, keyboard navigation, alt text, consistent colors.
  - Improve default Plotly themes; add dark/light mapping.
- Acceptance:
  - Manual checks on common laptop widths; no clipped content.

Sprint C16 — Tests & CI
- Tasks:
  - Add tests/ui/test_data_access.py and tests/ui/test_pages_import.py with fixtures.
  - Wire CI job to run UI tests in headless mode (no network).
- Acceptance:
  - pytest -q green locally and in CI.

Sprint C17 — Docs & Screenshots
- Tasks:
  - docs/DASHBOARD.md with instructions and screenshots/GIFs.
  - Update README.md and docs/MANUAL.html with a “Phase 3 Dashboard” section.
- Acceptance:
  - Docs build and link correctly; images render.

Sprint C18 — Release Wrap
- Tasks:
  - Final smoke: Overview, Backtests (single + compare), Live Monitor, Strategy Lab, Settings, (Tutor Viewer if present).
  - Tag and changelog entry; note view-only scope and safe defaults.
- Acceptance:
  - All Phase 3 Definition of Done checks pass.


Phase 4 — Portfolio, Automated ML Loop, and LLM “Brain”
Goals

Portfolio: robust multi-asset allocation with constraints and cost-aware rebalancing.
Automated ML loop: daily deterministic training → validation gates → safe promotion (staging/champion) → online monitoring → one-command rollback.
LLM “Brain”: propose bounded parameter tweaks; proposals must pass the same gates and require explicit approval for live use.
Scope and principles

Determinism and atomicity: every run writes to its own timestamped folder; only after all artifacts exist do we update pointers/symlinks.
File-system model registry: current (champion), staging (candidate), historical.
Shadow-first deployment: staging evaluated online with zero order impact until promoted explicitly or canary metrics pass.
Single source of truth for gates: both model promotions and LLM parameter proposals pass through the same validator (no shortcuts).
CI-friendly: no network calls; stub LLM and tiny fixture datasets.
Architecture (modules and CLIs)

Portfolio (carry-over, unchanged in spirit)

logos/portfolio/
allocator.py (HRP, risk parity, mean-variance w/ Ledoit–Wolf)
risk_model.py (realized/EWMA vol, shrinkage covariance)
optimizer.py (box/turnover constraints; heuristic fallback)
constraints.py (exposure caps, sector limits, max position/turnover)
position_manager.py (target → orders diff; slippage/fees-aware)
CLI: python -m logos.portfolio backtest …
Automated ML loop (new focus)

logos/alpha/ml/
train.py (or train_meta_model.py): deterministic training; atomic run dir; gates
promote.py: explicit champion switch with audit
monitor.py: online model/system health; writes status.json
rollback.py: repoint champion to prior run with audit
features.py, labels.py, pipelines.py, models.py, evaluation.py (from earlier plan; now integrated)
CLIs
python -m logos.alpha.ml.train --config configs/ml/daily.yaml
python -m logos.alpha.ml.promote --from runs/ml/<ts>/ --to data/cache/models/current --i-acknowledge-risk
python -m logos.alpha.ml.monitor --once
python -m logos.alpha.ml.rollback --to runs/ml/<ts_prev>/
LLM “Brain” (proposal engine)

logos/brain/
schemas.py (Proposal: deltas within bounds, rationale, confidence, risk flags)
policies.py (bounds, max delta, cooldowns, budgets)
prompts/, llm_agent.py (provider-agnostic; stub for CI; retries/backoff)
proposer.py (collects latest metrics; emits proposal.yaml and explain.md)
validator.py (shared gates with ML loop; quick backtest sims on tiny data)
scheduler.py (cron-friendly; sandbox-only by default)
CLIs
python -m logos.brain propose --strategy <name> --paper --max-delta 10% --explain
python -m logos.brain apply --proposal runs/brain/proposals/<ts>/proposal.yaml --paper | --live --i-acknowledge-risk
Paths & files (registry, atomic runs, monitoring)

data/cache/models/
current/ -> runs/ml/<champion_ts>/ (symlink)
staging/ -> runs/ml/<candidate_ts>/ (symlink)
historical stored under runs/ml/
runs/ml/<YYYYMMDD_HHMMSS>/
features.parquet, model.pkl, metadata.json, metrics.json, validation.json, train.log
Write to a temp dir, then rename(); update staging symlink atomically on success
runs/ml/promotions/<YYYYMMDD_HHMMSS>/promotion.json (audit trail)
runs/monitoring/status.json (model-health + system-health; OK/WARN/CRIT with reasons)
runs/brain/proposals/<YYYYMMDD_HHMMSS>_{scope}/
proposal.yaml, explain.md, validation.json, sim/*, logs/run.log
logos/logs/ml_train.log (stdout/stderr capture from cron/systemd)
Scheduler (daily automation)

Cron/systemd recommended. Example (daily 03:10 UTC):
10 3 * * * /home/ubuntu/.venv/bin/python -m logos.alpha.ml.train --config configs/ml/daily.yaml >> logos/logs/ml_train.log 2>&1
Training contract and gates (deterministic)

Train script responsibilities:
Fix RNG seeds and time splits (walk-forward or last-N-days OOS)
Emit:
metrics.json (train/valid/OOS: AUC/Acc for cls; IC/R²/MSE; OOS Sharpe of signal)
metadata.json (git SHA, data span, feature list + hashes, label schema, seeds, versions)
validation.json (gate pass/fail, deltas vs champion, comments)
Write to temp dir then rename atomically to runs/ml/<ts>
On success, atomically point data/cache/models/staging -> runs/ml/<ts>
Example gates vs champion (tunable in config):
Sharpe_oos >= champion_Sharpe_oos + 0.05
MaxDD_oos <= champion_MaxDD_oos + 2%
IC_oos >= 0.02
Stability: PSI for key features < 0.2
Sanity: at least N_eff signals/trades in OOS
If gates pass → create promotion_ticket.json (or flag in validation.json)
Shadow/canary and promotion (safe by default)

Live runner supports shadow eval:
Load both champion (current/) and staging (staging/) models
Log both signals; champion alone affects orders
Canary window: require staging’s real-time proxy metrics to be healthy (Sharpe on recent bars, bounded disagreement rate, etc.)
Promotion (explicit):
python -m logos.alpha.ml.promote --from runs/ml/<ts>/ --to data/cache/models/current --i-acknowledge-risk
Writes runs/ml/promotions/<ts_now>/promotion.json (who/when/why, deltas, gate results)
Monitoring and alerting

logos.alpha.ml.monitor produces runs/monitoring/status.json with:
Model-health:
Feature drift (PSI/KS vs training dist)
Prediction drift (score mean/variance; calibration: Brier for cls; MAE for reg)
Live OOS proxy metrics (rolling Sharpe/IC over last K bars)
Disagreement vs champion (flip rate)
System-health:
Data freshness and completeness
Latency breakdowns (polling → features → predict → decide)
State mismatches, broker rejects/timeouts
Heartbeats (runner + monitor liveness timestamps)
Levels: OK, WARN, CRIT; append reasons list. Optional email/Slack later; file-first now.
Rollback (one command)

Keep last N (e.g., 5) champions.
python -m logos.alpha.ml.rollback --to runs/ml/<ts_prev>/
Atomically repoints current/ to the chosen run; writes an audit entry.
LLM “Brain” integration changes (governance tightened)

All parameter proposals must:
Stay within policies.py bounds (min/max, max step size)
Respect budgets (tokens/day) and cooldowns
Pass the same validator gates on a quick backtest before any apply
Default to sandbox (proposal artifacts only); live apply requires explicit CLI flags
Shadow evaluation: if applied to a paper live session, log “champion vs proposal-adjusted” signals side-by-side before enabling real effect.
Tests

Unit:
Feature windowing and label alignment (no look-ahead), deterministic seeds
PSI/KS calculators; calibration metrics; gate logic (pass/fail around thresholds)
Portfolio: constraint satisfaction, turnover/cost handling
Brain: schema/bounds validation; stub LLM parser
Integration:
ML: end-to-end train on tiny fixture → artifacts + validation.json
Shadow: runner logs champion and staging signals; promotion flips effective model
Brain: propose → validate → sim artifacts; apply blocked without gates
Golden tests:
Tiny dataset with frozen seeds and versions; assert identical metrics.json across runs
CI:
No network; stub LLM provider; tiny fixtures; all tests pass via pytest -q
UI (Phase 3 extension; read-only)

Add pages/cards for:
Models: show current/staging pointers, latest metrics, validation gates
Monitoring: render runs/monitoring/status.json (badges for OK/WARN/CRIT)
Brain: proposals list and drill-down (rationale, bounds, validation)
No write actions from UI.
Documentation

docs/PORTFOLIO.md (allocators, constraints, CLI)
docs/ALPHA_ML.md (training contract, gates, registry, shadow, promotion, rollback)
docs/BRAIN.md (policies, governance, prompts, proposal lifecycle)
Update README/MANUAL with a “Phase 4 Automated ML Loop” section and safety checklist

Definition of Done (Phase 4)

Daily cron/systemd trains deterministically and writes complete artifacts to runs/ml/<ts>; staging symlink updated atomically on pass.
Shadow mode live runner logs champion and staging; only champion affects orders until explicit promotion or canary success.
Promote/rollback CLIs repoint symlinks atomically and write audits.
Monitor writes runs/monitoring/status.json with model vs system breakdown; WARN/CRIT levels trigger clear reasons.
LLM “Brain” proposals are bounded, validated by the same gates, and default to sandbox; no secrets leaked in artifacts.
Portfolio backtest CLI produces portfolio artifacts and metrics as in Phase 4 Part A.
CI passes with stubs and fixtures; no external network.
Smaller, focused sprints (1–2 days each)

Track A — Portfolio

A0 Risk primitives (realized/EWMA vol, Ledoit–Wolf) + tests
A1 Allocators v1 (risk parity, MV) + box constraints
A2 Turnover/cost-aware rebalance + order diffs
A3 Portfolio backtest CLI + artifacts and metrics
Track B — Automated ML loop

B0 Train contract + atomic run dir + gating; write staging symlink
B1 Model registry symlinks (current/staging) + promotion audit
B2 Shadow wiring in live runner (dual predictions, champion-only orders)
B3 Monitor (PSI/KS, calibration, proxy metrics, system health) → status.json
B4 Rollback CLI and tests
B5 Golden-tests and determinism hardening (fixtures, seeds, versions)
B6 Cron/systemd job + docs + ml_train.log capture
Track C — LLM “Brain”

C0 Policies/bounds schema + proposer/validator integration (shared gates)
C1 Stub LLM + prompts; proposal artifacts; sandbox-only
C2 Optional paper auto-apply after gates; live apply requires explicit flags
Track D — UI & Docs

D0 Dashboard extensions (Models, Monitoring, Brain) read-only
D1 PORTFOLIO.md, ALPHA_ML.md, BRAIN.md; README/MANUAL updates