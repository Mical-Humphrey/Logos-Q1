Phase 2
========

Implementation Plan
- Order generation: build the strategy-to-order translator that respects symbol metadata, precision, notional caps, and sizing rules.
- Paper broker completion: implement deterministic fills, FIFO inventory, realized/unrealized PnL, order-state lifecycle, and event logging.
- Data feeds: deliver minute-bar feeds (cache replay for tests, yfinance polling for equities, CCXT public polling for crypto) with freshness checks and retry/backoff.
- Risk and circuit breakers: wire max-notional, exposure, drawdown, reject, stale-data, and kill-switch guards into the runner with focused tests.
- State and reporting: persist state.json/state.jsonl for restart support, reconcile open orders, and emit full session artifacts (orders.csv, trades.csv, positions.csv, account.csv, session.md).
- Runner integration: connect the completed components and prove an end-to-end paper session using deterministic fixtures.
- Broker adapters: finalize CCXT and Alpaca connectors for dry runs and credential validation, leaving IB as a documented stub.
- Docs and runbook: refresh README, manual, and live runbook to reflect the finished live workflow.
- Test suite: keep `pytest -q` passing with the new live-mode coverage and without network credentials.

Definition of Done
- `python -m logos.live trade ...` runs end-to-end in paper mode using deterministic feeds, produces the full session artifact set, and exercises all risk guards (kill switch, breaker thresholds, restart flow) with test coverage.
- Strategy order generation reuses backtest signals, emitting quantized `OrderIntent`s that respect market metadata and sizing caps.
- Paper broker, risk module, and data feeds are covered by unit tests; CCXT and Alpaca adapters validate configuration and support dry-run execution paths.
- The config validator blocks unsafe live runs and verifies credential readiness for live adapters.
- Documentation and runbook describe the live workflow, safety checklist, and recovery steps.
- Full test suite passes locally without relying on external networks or real credentials.

Sprint Plan

Sprint A (Core Loop Completion)
- Implement and test strategy-to-order translation.
- Finish paper broker fill logic, inventory tracking, and PnL accounting.
- Build deterministic minute-bar feeds (cache replay and mockable providers), including freshness checks and retries.
- Integrate risk/circuit breakers with targeted tests.
- Finalize state persistence, restart reconciliation, and artifact writers.
- Demonstrate an end-to-end paper session using deterministic inputs, producing the expected artifacts.

Sprint B (Adapters & Polish)
- Complete CCXT and Alpaca dry-run adapters with credential validation and error handling; document IB as a future task.
- Expand reporting (session markdown, summaries) and ensure config validator enforces safety toggles.
- Refresh documentation/runbook to match the finished live pipeline.
- Harden tests and CI workflows around live-mode features, ensuring `pytest` stays green in isolation.



Phase 3 — Visual Dashboard (Streamlit + Plotly)
===============================================

Goal
- Build a read-only Streamlit dashboard that surfaces backtests, live sessions, metrics, and logs from the filesystem. No trading actions in Phase 3 (view-only). The UI must be modular, fast, and safe to run locally.

Scope
- Use Streamlit for the GUI, Plotly for charts, and import canonical paths from logos.paths (add any missing helpers).
- Read artifacts from runs/, data/cache/, and logos/logs/ only; never mutate artifacts from the UI.

Implementation Plan (Read-only UI)
- Package: logos/ui/streamlit/
  - __init__.py
  - app.py (main entry; streamlit run logos/ui/streamlit/app.py)
  - state.py (session_state: selected symbol/run, refresh interval, theme)
  - data_access.py (pure readers using logos.paths; legacy-aware)
  - components/
    - metrics_card.py (CAGR, Sharpe, MaxDD, WinRate, Exposure)
    - equity_chart.py (Plotly equity + drawdown overlay)
    - trades_table.py (paginated trades view with filters/download)
    - log_viewer.py (tail file with autorefresh + regex filter)
  - pages/
    - 00_Overview.py (KPI tiles, recent runs, latest live session)
    - 10_Backtests.py (run explorer; compare runs)
    - 20_Live_Monitor.py (session status, PnL, positions, fills, log tail)
    - 30_Strategy_Lab.py (strategy docs/param schema; no execute)
    - 40_Settings.py (safe .env/config view; theme + refresh)
    - (Optional) 50_Tutor_Viewer.py (render lessons transcript/plots)

Paths & Files (read-only; align with Phase 2)
- Backtests: runs/<backtest_id>/{metrics.json, trades.csv, equity.png, config.yaml, logs/run.log}
  - Legacy equity images: runs/equity_{SYMBOL}_{STRAT}.png (display if present)
- Live: runs/live/sessions/<id>/{orders.csv, trades.csv, positions.csv, account.csv, session.md, plots/*.png, logs/run.log, state.json}
  - runs/live/latest_session (symlink or pointer file)
- Logs: logos/logs/live.log and logos/logs/app.log
- Tutor: runs/lessons/<lesson>/<timestamp>/{transcript.md|.txt, glossary.json, plots/*.png}

Performance & Quality
- Backtests page loads under 500ms for typical runs (<50k rows).
- Live Monitor avoids full rereads; cache by mtime and use small deltas.
- Handle partial file writes without crashing; show retry/empty-state.
- Accessibility: responsive layout, keyboard navigation, alt text, consistent colors.
- Security: never display secrets; redact any values with KEY/SECRET/TOKEN.

Testing
- tests/ui/test_data_access.py: validate loaders on fixtures in tests/fixtures/runs/…
- tests/ui/test_pages_import.py: import all pages and assert layout functions render without raising using fixtures/mocks.

Documentation
- docs/DASHBOARD.md: how to run, screenshots per page, file/metrics expectations and empty-state behavior.
- Update README.md and docs/MANUAL.html with “Phase 3 Dashboard” section and a short gif/screenshot.

Definition of Done (Phase 3)
- streamlit run logos/ui/streamlit/app.py launches without errors.
- Overview shows KPIs, recent backtests, and (if present) latest live session with PnL and log tail.
- Backtests page: run picker, metrics cards, equity with drawdown, trades table with filters/download, compare mode with overlay + metric deltas.
- Live Monitor: with a dummy/recorded session fixture, displays account snapshot, positions, trades, and live.log tail; autorefresh updates PnL chart and log tail.
- Strategy Lab lists strategies from logos/strategies and renders docstrings/param schemas and example CLIs.
- Settings shows redacted env/config and allows changing UI refresh interval and theme.
- Missing paths or empty runs produce clear empty-state cards—no stack traces.
- Tests pass: pytest -q (UI tests + existing suite).
- Docs updated: docs/DASHBOARD.md plus README & MANUAL references.

Smaller, Focused Sprints (1–2 days each)

Sprint C0 — Prep & Skeleton
- Tasks:
  - Add Streamlit and Plotly to requirements if missing.
  - Scaffold logos/ui/streamlit/{__init__.py, app.py, state.py, data_access.py, components/, pages/}.
  - Ensure logos.paths exposes helpers for RUNS_DIR, LOGOS_DIR/logs, etc.
- Acceptance:
  - streamlit run logos/ui/streamlit/app.py shows a placeholder home page.
  - Imports from logos.paths succeed.

Sprint C1 — Data Access: Backtests (Base)
- Tasks:
  - Implement list_backtests(), load_backtest_metrics(), load_backtest_equity().
  - Handle standardized and legacy filenames; warn on legacy.
- Acceptance:
  - A simple script loads metrics/equity for a fixture run without errors.
  - Unit tests for these functions pass.

Sprint C2 — Data Access: Trades + Live Stubs
- Tasks:
  - Implement load_backtest_trades().
  - Stub list_live_sessions() and load_live_snapshot() returning empty frames when absent.
- Acceptance:
  - Trades load for fixture; live stubs return empty-state safely.
  - Tests updated and passing.

Sprint C3 — Components v1: Metrics + Equity
- Tasks:
  - Build components/metrics_card.py (CAGR, Sharpe, MaxDD, WinRate, Exposure).
  - Build components/equity_chart.py with drawdown overlay (Plotly).
- Acceptance:
  - Demo page renders metrics cards and equity chart from fixtures under 500ms.

Sprint C4 — Components v2: Trades Table + Log Viewer
- Tasks:
  - Build trades_table.py (pagination, side/date filters, CSV download).
  - Build log_viewer.py (tail last N lines with optional regex filter).
- Acceptance:
  - Demo page shows filtered trades; download button saves CSV; log tail works on a sample log.

Sprint C5 — Page: 00_Overview (MVP)
- Tasks:
  - KPI tiles: # backtests, # strategies (from filesystem), last run time.
  - Recent backtests table.
  - Latest live session panel (empty-state if none).
- Acceptance:
  - Overview renders with fixtures; empty-state shows friendly text.

Sprint C6 — Page: 10_Backtests (Single Run)
- Tasks:
  - Sidebar filters (symbol/strategy/date) and run picker.
  - Show metrics cards + equity chart for selected run.
- Acceptance:
  - Selecting a run updates cards and chart; no compare yet.

Sprint C7 — Page: 10_Backtests (Compare Mode)
- Tasks:
  - Add “Compare runs” toggle; select Run A and Run B.
  - Overlay equities; show side-by-side metric deltas.
- Acceptance:
  - Overlay works smoothly; metrics deltas are correct for fixtures.

Sprint C8 — Live Data Access (Real)
- Tasks:
  - Implement list_live_sessions() and load_live_snapshot() using Phase 2 paths (orders.csv, trades.csv, positions.csv, account.csv).
  - Implement tail_log() with safe partial-read handling; mtime-based caching.
- Acceptance:
  - With live fixtures, returns data frames and log lines quickly and safely.

Sprint C9 — Page: 20_Live_Monitor (MVP)
- Tasks:
  - Panels: account snapshot, open positions, recent fills, log tail.
  - Autorefresh interval in state (2/5/10s); show last refresh time.
- Acceptance:
  - Live page updates panels with fixtures; no PnL chart yet.

Sprint C10 — Live Monitor: PnL Timeline
- Tasks:
  - Compute PnL time series from trades/positions; Plotly line.
  - Cache with short TTL; avoid full rereads each tick.
- Acceptance:
  - PnL timeline updates on refresh; performance under 500ms with fixtures.

Sprint C11 — Page: 30_Strategy_Lab
- Tasks:
  - Enumerate strategies from logos/strategies; render docstrings and param schema; example CLIs.
- Acceptance:
  - Strategies list appears; params render; no execute button.

Sprint C12 — Page: 40_Settings
- Tasks:
  - Show safe subset of .env and logos.config (redact KEY/SECRET/TOKEN).
  - Theme toggle and refresh interval controls.
- Acceptance:
  - Redactions verified; state persists within session.

Sprint C13 — (Optional) 50_Tutor_Viewer
- Tasks:
  - Render transcript (.md or .txt) and plots from runs/lessons/<lesson>/<ts>/.
  - Graceful empty-state if lessons absent.
- Acceptance:
  - Viewer displays lesson artifacts for fixtures.

Sprint C14 — Caching & Partial-Write Hardening
- Tasks:
  - st.cache_data with TTLs (2–5s live); invalidate by file mtime.
  - Defensive reads (retry-on-partial, consistent parsing).
- Acceptance:
  - No crashes during simulated file writes; logs show retries.

Sprint C15 — Accessibility & Layout Polish
- Tasks:
  - Responsive layout, keyboard navigation, alt text, consistent colors.
  - Improve default Plotly themes; add dark/light mapping.
- Acceptance:
  - Manual checks on common laptop widths; no clipped content.

Sprint C16 — Tests & CI
- Tasks:
  - Add tests/ui/test_data_access.py and tests/ui/test_pages_import.py with fixtures.
  - Wire CI job to run UI tests in headless mode (no network).
- Acceptance:
  - pytest -q green locally and in CI.

Sprint C17 — Docs & Screenshots
- Tasks:
  - docs/DASHBOARD.md with instructions and screenshots/GIFs.
  - Update README.md and docs/MANUAL.html with a “Phase 3 Dashboard” section.
- Acceptance:
  - Docs build and link correctly; images render.

Sprint C18 — Release Wrap
- Tasks:
  - Final smoke: Overview, Backtests (single + compare), Live Monitor, Strategy Lab, Settings, (Tutor Viewer if present).
  - Tag and changelog entry; note view-only scope and safe defaults.
- Acceptance:
  - All Phase 3 Definition of Done checks pass.


  Phase 4 — Portfolio, ML Alpha Research, and LLM “Brain”
Goal

Add a portfolio engine, advanced risk modeling, and a governed LLM “Brain” that proposes bounded parameter tweaks. Preserve determinism, enforce guardrails, and require approvals for any live change.
Scope

Portfolio construction: multi-asset allocation, constraints, volatility targeting, turnover and cost-aware rebalancing.
Risk modeling: covariance estimators (EWMA, Ledoit-Wolf), drawdown controls, capital allocation limits, exposure caps.
ML alpha research: feature pipelines, label schemas, model training/evaluation with walk-forward/TSCV; strictly offline/deterministic by default.
LLM Brain: periodic proposals for parameter tweaks within boundaries; proposals saved as artifacts and applied only after gates (backtest checks) and human approval (default).
Architecture (new packages/modules)

logos/portfolio/
allocator.py (HRP, mean-variance, risk parity; constraint handling)
risk_model.py (realized vol, EWMA vol, Ledoit-Wolf covariances, beta)
optimizer.py (quadratic programming with box/turnover constraints; heuristic fallback)
constraints.py (exposure caps, sector/asset-class limits, max position, max turnover)
position_manager.py (target → orders diff for live/backtest; rebalancing cadence)
logos/alpha/ml/
features.py (returns, z-scores, momentum windows, volatility, rolling stats)
labels.py (next-period return, classification +/- with thresholds)
pipelines.py (fit/transform pipeline; feature scaling; leakage guards)
models.py (sklearn baselines: LogisticRegression, RandomForest; optional XGBoost if present)
evaluation.py (walk-forward, time-series CV, OOS metrics, feature importance summary)
logos/brain/
schemas.py (Pydantic proposal schema: bounds, rationale, confidence, risk flags)
prompts/ (prompt templates; versioned)
policies.py (parameter bounds, rate limits, budget)
llm_agent.py (provider-agnostic LLM client; retries; redaction; budget tracking)
proposer.py (collects recent metrics/runs; forms question; parses response → Proposal)
validator.py (sanity checks, bounds enforcement, backtest gates; risk/circuit-breakers)
scheduler.py (cron-like loop; writes proposals; optional auto-apply in paper mode)
state.py (JSON state: last call time, budget, cooldowns)
CLI
python -m logos.portfolio backtest ... (portfolio-level backtest)
python -m logos.alpha.ml train/eval ...
python -m logos.brain propose --strategy mean_reversion --paper --max-delta 10% --explain
python -m logos.brain apply --proposal <path> [--paper | --live --i-acknowledge-risk]
Paths & Files (align with Phase 2/3; add new)

New data/cache:
data/cache/features/ (feature matrices; parquet/csv)
data/cache/models/ (trained models; joblib/pickle with metadata.json)
Portfolio runs:
runs/portfolio/<id>/{config.yaml, metrics.json, weights.csv, trades.csv, equity.png, logs/run.log}
Brain artifacts:
runs/brain/proposals/<YYYY-MM-DD_HHMM>_{scope}/
proposal.yaml (strategy params deltas; rationale; bounds; confidence)
explain.md (LLM explanation; redacted)
validation.json (gates result; metrics deltas; risk flags)
sim/ (quick backtest artifacts for the proposal)
logs/run.log
runs/brain/latest (pointer)
Env/Secrets
.env.example: OPENAI_API_KEY or provider keys; BRAIN_BUDGET_TOKENS_PER_DAY; BRAIN_COOLDOWN_SECS; BRAIN_PROVIDER
logos/paths helpers to add (read-only for UI; write used by engines):
RUNS_PORTFOLIO_DIR = RUNS_DIR / "portfolio"
RUNS_BRAIN_DIR = RUNS_DIR / "brain"
RUNS_BRAIN_PROPOSALS_DIR = RUNS_BRAIN_DIR / "proposals"
Security, Safety, and Governance

Default mode is sandbox: proposals only; no auto-apply.
Hard bounds on parameters (policies.py) with per-parameter min/max, max step size, and rate-limited changes.
Budgeting: tokens/day and cooldowns; emergency kill-switch.
No secrets in logs; redact prompts/responses; store hashes only if needed.
Human-in-the-loop required for live apply; paper mode can auto-apply after gates pass.
Implementation Plan

Portfolio engine (multi-asset)
Allocator: HRP, risk parity, and mean-variance with Ledoit-Wolf covariance.
Constraints: long-only/box limits, per-asset max weight, sector caps, turnover limit.
Risk model: realized vol, EWMA(λ configurable), Ledoit-Wolf shrinkage, drawdown estimator.
Position manager: compare current weights to targets; produce orders with notional/lot sizing; respect max turnover/costs.
CLI backtest producing portfolio run artifacts.
Acceptance:
Can backtest 3+ assets with deterministic small datasets; weights.csv matches constraints; metrics.json includes Portfolio Sharpe, MaxDD, Turnover, AvgWeight, TrackingError.
ML alpha research (deterministic)
Feature/label pipelines: minimal leakage-safe windows, standard scaling, train/valid/test splits by time, walk-forward.
Models: sklearn baselines with fixed seeds; optional XGBoost behind an extra requirement.
Evaluation: OOS metrics (AUC, accuracy for classification; IC, Sharpe of signal), plot importance (if available), export model + metadata.
Artifacts to data/cache/models/ and runs/<id> for experiments.
Acceptance:
Deterministic training on fixture data; serialized model with metadata.json; evaluation report in metrics.json; tests verify feature windowing and determinism.
Brain (LLM parameter proposer)
Schema: Proposal with fields: scope (strategy or portfolio), params_before, params_after (deltas), bounds_applied, rationale, confidence, risk_flags, created_at, version.
Policies: bounds per param; max-delta; daily token budget; cooldown; allow/deny-lists.
LLM client (provider-agnostic): supports OPENAI-like API; retries/backoff; timeouts; token accounting; redaction.
Proposer: collect latest metrics/runs, prepare context window, prompt templates in prompts/, parse JSON response strictly into Proposal; on parse fail → reject.
Validator: enforce bounds; run quick deterministic backtest on tiny dataset to compare baseline vs proposal; require gates e.g. Sharpe_delta >= 0.05, MaxDD not worse than threshold; exposure sanity checks.
Scheduler: cron-like loop; by default sandbox-only writes artifacts; optional auto-apply in paper mode when gates pass; live applies require CLI with explicit flags.
Artifacts under runs/brain/proposals/... as described.
Acceptance:
CLI propose writes proposal.yaml + explain.md + validation.json + sim artifacts; proposals respect bounds; validator blocks unsafe suggestions; sandbox mode enabled by default.
Live integration hooks (read-only until approved)
Live runner reads an “effective config” layer that can accept an approved proposal.yaml to override parameters at session start (not mid-bar).
Audit: effective config written to runs/live/sessions/<id>/config.yaml with proposal_ref if applied.
Acceptance:
In paper mode, apply --proposal <path> on session start and log overrides; live requires --i-acknowledge-risk.
UI (Phase 3 extension, read-only)
Add dashboard tiles for Portfolio and Brain:
Portfolio: list portfolio runs; show weights table + equity.
Brain: proposals list with status (proposed/validated/approved/applied); render explain.md, bounds applied, and validation metrics.
Acceptance:
Overview shows counts; Backtests/Portfolio pages display artifacts; Brain page shows proposal drill-down. No write actions.
Performance & Quality

Deterministic seeds; fixtures; no network in CI (Brain provides stub LLM that returns fixed proposal for tests).
Portfolio optimizer must handle small N (<=50 assets) in <500ms on fixtures; graceful fallback if solver not present.
ML pipeline avoids data leakage; caches features deterministically; documents provenance in metadata.json.
Brain proposals are small bounded diffs; prompt templates versioned.
Testing

Unit tests:
Portfolio: constraint satisfaction (weights sum to 1, box limits respected), turnover limiter, cost-aware order diffs.
Risk model: EWMA and Ledoit-Wolf sanity; max drawdown calc consistent with existing metrics.
ML: feature windowing correctness; deterministic model training; evaluation metrics in expected ranges on fixtures.
Brain: proposal schema validation; bounds enforcement; stub LLM parser; validator gating (accept/reject logic); budget/cooldown enforcement.
Integration tests:
Portfolio backtest end-to-end produces full artifact set.
Brain propose → validate → artifacts written; optional apply in paper session writes proposal_ref in config.
CI:
No real LLM/network calls; use stub provider.
All tests pass: pytest -q.
Documentation

docs/PORTFOLIO.md: allocators, risk models, constraints, CLI usage, artifacts.
docs/ALPHA_ML.md: feature/label design, models, evaluation, determinism.
docs/BRAIN.md: governance, bounds, budgets, prompts, proposal lifecycle, CLI examples, approval workflow and safety checklist.
Update README and MANUAL with Phase 4 sections and safety notes.
Definition of Done (Phase 4)

Portfolio backtest CLI runs on fixtures and writes: config.yaml, metrics.json, weights.csv, trades.csv, equity.png, logs/run.log under runs/portfolio/<id>.
ML pipelines train deterministically on fixtures; serialize models with metadata; evaluation metrics emitted.
Brain proposer generates bounded proposals, validates via quick backtest gates, and persists artifacts; default sandbox mode; no secrets leaked.
Live runner can start with an approved proposal.yaml in paper mode and records proposal_ref in effective config; live requires explicit flags.
Streamlit dashboard (Phase 3) shows portfolio runs and brain proposals read-only.
All tests pass locally and in CI without network calls.
Smaller, Focused Sprints (1–2 days each)

Sprint D0 — Portfolio Skeleton

Files: allocator.py, risk_model.py, constraints.py, optimizer.py, position_manager.py; CLI stub.
Acceptance: weights computation pipeline skeleton; unit tests compile.
Sprint D1 — Risk Model Primitives

Implement realized/EWMA vol, Ledoit-Wolf covariance; tests with synthetic returns.
Acceptance: correct shapes, positive semidefinite covariance, deterministic outputs.
Sprint D2 — Allocators v1 (Risk Parity + MV)

Implement risk parity and mean-variance with shrinkage; box constraints.
Acceptance: weights sum to ~1, within bounds; simple 3-asset fixture passes.
Sprint D3 — Turnover & Cost-Aware Rebalance

Position diff with turnover limit and cost slippage inputs.
Acceptance: rebalancing respects max turnover and cost penalties.
Sprint D4 — Portfolio Backtest CLI + Artifacts

End-to-end backtest producing runs/portfolio/<id> artifacts.
Acceptance: metrics.json includes Portfolio Sharpe, MaxDD, Turnover; weights.csv written.
Sprint D5 — ML Feature/Label Pipelines

features.py, labels.py, pipelines.py with leakage guards.
Acceptance: deterministic feature matrices on fixtures; tests pass.
Sprint D6 — Models + Evaluation

models.py (sklearn baselines) and evaluation.py (walk-forward / TSCV).
Acceptance: serialized model + metadata; OOS metrics computed; tests pass.
Sprint D7 — Brain Schemas & Policies

Pydantic proposal schema; bounds config; budget/cooldown tracking.
Acceptance: schema validates; bounds enforced; tests for deltas.
Sprint D8 — LLM Client + Prompts (Stub + Provider)

llm_agent.py with provider-agnostic interface; stub provider for CI; prompt templates.
Acceptance: stub returns deterministic proposal; no secrets in logs.
Sprint D9 — Proposer + Validator + Artifacts

proposer.py, validator.py; write proposal.yaml, explain.md, validation.json, sim/ artifacts.
Acceptance: proposals generated and validated on fixtures; failing gates reject.
Sprint D10 — Scheduler & CLI

scheduler.py loop; CLI for propose/apply; paper-mode auto-apply optional.
Acceptance: cron-style run produces new proposal folders; apply respects flags and approval.
Sprint D11 — Live Hook (Read-Only Apply)

Runner reads approved proposal at session start; records proposal_ref; no mid-session changes.
Acceptance: paper session loads overrides from proposal; logs override summary.
Sprint D12 — UI Extension (Read-Only)

Streamlit pages: Portfolio viewer, Brain proposals list/drill-down.
Acceptance: pages render fixtures; no write actions.
Sprint D13 — Docs & Runbook

PORTFOLIO.md, ALPHA_ML.md, BRAIN.md; safety checklist and approval workflow.
Acceptance: docs build; links from README/MANUAL.
Sprint D14 — Final QA & Release

End-to-end smoke with fixtures: portfolio backtest, ML training, brain proposal, UI view.
Acceptance: Definition of Done met; tests/CI green; tag release notes.
Non-Goals (Phase 4)

No automatic live parameter changes without human approval by default.
No deep learning stacks; keep ML to classical models unless explicitly enabled.
No persistent external DB; continue using filesystem artifacts and JSON/CSV for reproducibility.